{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b24f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¥ Load real Swiss Post data with simulated performance issues\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data4day_with_issues.csv\")\n",
    "df[\"HOUR_TIME\"] = pd.to_datetime(df[\"HOUR_TIME\"])\n",
    "df[\"HOUR\"] = df[\"HOUR_TIME\"].dt.hour\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“Š Summarize chute-level stats\n",
    "summary = (\n",
    "    df.groupby([\"CHUTE\", \"HOUR\"])\n",
    "    .agg(\n",
    "        CHUTE_LOAD=(\"PACKAGE_COUNT\", \"sum\"),\n",
    "        ZIP_COUNT=(\"ZIP_CODE\", \"nunique\"),\n",
    "        AVG_DELAY=(\"AVG_PROCESSING_TIME_MINUTES\", \"mean\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "max_load = summary[\"CHUTE_LOAD\"].max()\n",
    "summary[\"CHUTE_LOAD_NORM\"] = summary[\"CHUTE_LOAD\"] / max_load * 2\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ§  Prepare states for RL environment\n",
    "sim_states = []\n",
    "for _, row in summary.iterrows():\n",
    "    sim_states.append({\n",
    "        \"chute\": row[\"CHUTE\"],\n",
    "        \"hour\": int(row[\"HOUR\"]),\n",
    "        \"load\": float(row[\"CHUTE_LOAD_NORM\"]),\n",
    "        \"zip_count\": int(row[\"ZIP_COUNT\"]),\n",
    "        \"avg_delay\": float(row[\"AVG_DELAY\"])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22176b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PackagingCenterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Enhanced Packaging Center Environment with realistic contention simulation\n",
    "    \"\"\"\n",
    "    def __init__(self, max_chutes=5, peak_hours=[9, 12, 17], holiday_mode=False):\n",
    "        super(PackagingCenterEnv, self).__init__()\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.max_chutes = max_chutes\n",
    "        self.peak_hours = peak_hours\n",
    "        self.holiday_mode = holiday_mode\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 100\n",
    "        \n",
    "        # State: [chute_load, hour_of_day, day_of_week, active_chutes, queue_length, processing_rate]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.1]),\n",
    "            high=np.array([2.0, 23.0, 6.0, float(max_chutes), 100.0, 2.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Actions: 0=nothing, 1=add_chute, 2=reroute_packages, 3=priority_processing\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Initialize tracking first\n",
    "        self.history = {\n",
    "            'load': [],\n",
    "            'queue': [],\n",
    "            'throughput': [],\n",
    "            'delays': [],\n",
    "            'actions': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.current_step = 0\n",
    "        self.current_hour = np.random.randint(0, 24)\n",
    "        self.current_day = np.random.randint(0, 7)\n",
    "        self.active_chutes = 2\n",
    "        self.queue_length = np.random.uniform(5, 15)\n",
    "        self.processing_rate = 1.0\n",
    "        self.chute_load = np.random.uniform(0.3, 0.7)\n",
    "        \n",
    "        # Clear history\n",
    "        for key in self.history:\n",
    "            self.history[key].clear()\n",
    "            \n",
    "        self.state = np.array([\n",
    "            self.chute_load,\n",
    "            self.current_hour,\n",
    "            self.current_day,\n",
    "            self.active_chutes,\n",
    "            self.queue_length,\n",
    "            self.processing_rate\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return self.state, {}\n",
    "    \n",
    "    def _get_demand_multiplier(self):\n",
    "        \"\"\"Calculate demand based on time patterns\"\"\"\n",
    "        base_demand = 1.0\n",
    "        \n",
    "        # Peak hours\n",
    "        if self.current_hour in self.peak_hours:\n",
    "            base_demand *= 1.5\n",
    "        \n",
    "        # Weekend reduction\n",
    "        if self.current_day in [5, 6]:  # Saturday, Sunday\n",
    "            base_demand *= 0.7\n",
    "        \n",
    "        # Holiday surge\n",
    "        if self.holiday_mode:\n",
    "            base_demand *= 2.0\n",
    "            \n",
    "        # Random variation\n",
    "        base_demand *= np.random.uniform(0.8, 1.2)\n",
    "        \n",
    "        return base_demand\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        old_load = self.chute_load\n",
    "        \n",
    "        # Get current demand\n",
    "        demand_mult = self._get_demand_multiplier()\n",
    "        incoming_packages = np.random.poisson(10 * demand_mult)\n",
    "        \n",
    "        # Apply action\n",
    "        action_cost = 0\n",
    "        if action == 1:  # Add chute\n",
    "            if self.active_chutes < self.max_chutes:\n",
    "                self.active_chutes += 1\n",
    "                action_cost = 5  # Cost of activating chute\n",
    "        elif action == 2:  # Reroute packages\n",
    "            self.queue_length *= 0.8  # Reduce queue by rerouting\n",
    "            action_cost = 2\n",
    "        elif action == 3:  # Priority processing\n",
    "            self.processing_rate = min(2.0, self.processing_rate * 1.3)\n",
    "            action_cost = 3\n",
    "        \n",
    "        # Process packages\n",
    "        processed = min(\n",
    "            self.queue_length,\n",
    "            self.active_chutes * self.processing_rate * 8  # packages per chute per hour\n",
    "        )\n",
    "        \n",
    "        # Update queue and load\n",
    "        self.queue_length = max(0, self.queue_length - processed + incoming_packages)\n",
    "        self.chute_load = min(2.0, self.queue_length / (self.active_chutes * 20))\n",
    "        \n",
    "        # Calculate delays and throughput\n",
    "        avg_delay = self.chute_load * (2.0 if self.chute_load > 1.0 else 1.0)\n",
    "        throughput = processed / max(1, incoming_packages)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = 0\n",
    "        # Penalize high chute load\n",
    "        if self.chute_load > 1.0:\n",
    "            reward -= (self.chute_load - 1.0) * 10\n",
    "        # Encourage action diversity\n",
    "        if action == 0:  # Do Nothing\n",
    "            reward -= 1\n",
    "        elif action == 1:  # Add Chute\n",
    "            reward += 1 if self.queue_length > 30 else -1\n",
    "        elif action == 2:  # Reroute Packages\n",
    "            reward += 2 if self.queue_length > 20 else -0.5\n",
    "        \n",
    "        # Throughput reward\n",
    "        reward += throughput * 10\n",
    "        \n",
    "        # Delay penalty\n",
    "        reward -= avg_delay * 5\n",
    "        \n",
    "        # Queue management\n",
    "        if self.queue_length > 50:\n",
    "            reward -= 10  # Heavy penalty for long queues\n",
    "        \n",
    "        # Load balancing\n",
    "        if 0.4 <= self.chute_load <= 0.8:\n",
    "            reward += 5  # Reward for optimal load\n",
    "        \n",
    "        # Action cost\n",
    "        reward -= action_cost\n",
    "        \n",
    "        # Efficiency bonus\n",
    "        if self.chute_load < old_load and action != 0:\n",
    "            reward += 3  # Reward for effective actions\n",
    "        \n",
    "        # Update processing rate (decay over time)\n",
    "        self.processing_rate = max(0.5, self.processing_rate * 0.95)\n",
    "        \n",
    "        # Time progression\n",
    "        self.current_hour = (self.current_hour + 1) % 24\n",
    "        if self.current_hour == 0:\n",
    "            self.current_day = (self.current_day + 1) % 7\n",
    "        \n",
    "        # Update state\n",
    "        self.state = np.array([\n",
    "            self.chute_load,\n",
    "            self.current_hour,\n",
    "            self.current_day,\n",
    "            self.active_chutes,\n",
    "            self.queue_length,\n",
    "            self.processing_rate\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Track history\n",
    "        self.history['load'].append(self.chute_load)\n",
    "        self.history['queue'].append(self.queue_length)\n",
    "        self.history['throughput'].append(throughput)\n",
    "        self.history['delays'].append(avg_delay)\n",
    "        self.history['actions'].append(action)\n",
    "        self.history['rewards'].append(reward)\n",
    "        \n",
    "        done = self.current_step >= self.max_steps\n",
    "        truncated = False\n",
    "        \n",
    "        return self.state, reward, done, truncated, {\n",
    "            'delay': avg_delay,\n",
    "            'throughput': throughput,\n",
    "            'queue_length': self.queue_length\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.locals.get(\"dones\", [False])[0]:\n",
    "            reward = sum(self.locals[\"infos\"][0].get(\"episode\", {}).get(\"r\", [0]))\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_lengths.append(self.num_timesteps)\n",
    "        return True\n",
    "\n",
    "reward_logger = RewardLoggerCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81165149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback to track training progress\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_episode_reward = 0\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Track rewards\n",
    "        if 'rewards' in self.locals:\n",
    "            self.current_episode_reward += self.locals['rewards'][0]\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if self.locals.get('dones', [False])[0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.current_episode_reward = 0\n",
    "        \n",
    "        return True\n",
    "\n",
    "def run_simulation(total_timesteps=10000, test_episodes=5):\n",
    "    \"\"\"Run the complete simulation\"\"\"\n",
    "    print(\"ðŸ­ Starting Packaging Center Contention Simulation\")\n",
    "    base_env = PackagingCenterEnv(max_chutes=5, peak_hours=[9, 12, 17])\n",
    "    vec_env = DummyVecEnv([lambda: base_env])\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create environment\n",
    "    \n",
    "    # Create and train model\n",
    "    print(\"ðŸ¤– Training RL Agent...\")\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1,\n",
    "        learning_rate=0.001,\n",
    "        buffer_size=10000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=32,\n",
    "        target_update_interval=100,\n",
    "        exploration_fraction=0.3,\n",
    "        exploration_final_eps=0.05\n",
    "    )\n",
    "    \n",
    "    callback = TrainingCallback()\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    print(\"âœ… Training completed!\")\n",
    "    \n",
    "    # Test the trained model\n",
    "    print(f\"\\nðŸ§ª Testing trained model for {test_episodes} episodes...\")\n",
    "    \n",
    "    test_results = []\n",
    "    for episode in range(test_episodes):\n",
    "        obs = vec_env.reset()\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = obs[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            step_result = vec_env.step(action)\n",
    "            obs, reward, done = step_result[0], step_result[1], step_result[2]\n",
    "            \n",
    "            # Extract scalar reward value\n",
    "            if isinstance(reward, (list, tuple, np.ndarray)):\n",
    "                reward_val = float(reward[0])\n",
    "            else:\n",
    "                reward_val = float(reward)\n",
    "            \n",
    "            episode_reward += reward_val\n",
    "            step_count += 1\n",
    "            \n",
    "        # Ensure episode_reward is a scalar\n",
    "        episode_reward = float(episode_reward)\n",
    "        \n",
    "        test_results.append({\n",
    "            'episode': episode + 1,\n",
    "            'total_reward': episode_reward,\n",
    "            'steps': step_count,\n",
    "            'avg_reward_per_step': episode_reward / step_count\n",
    "        })\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {step_count}\")\n",
    "        # âœ… Manual simulation for tracking history\n",
    "        obs = base_env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = base_env.step(action)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    create_visualization(base_env, test_results)\n",
    "    \n",
    "    return model, base_env, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_visualization(base_env, test_results):\n",
    "    \"\"\"Create comprehensive visualization of results\"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Check if we have history data\n",
    "    has_history = len(base_env.history['load']) > 0\n",
    "    \n",
    "    if has_history:\n",
    "        # 1. Load vs Time\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        plt.plot(base_env.history['load'], color='red', linewidth=2)\n",
    "        plt.title('Chute Load Over Time', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Load Level')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Queue Length\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        plt.plot(base_env.history['queue'], color='orange', linewidth=2)\n",
    "        plt.title('Queue Length Over Time', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Packages in Queue')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Throughput\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        plt.plot(base_env.history['throughput'], color='green', linewidth=2)\n",
    "        plt.title('Throughput Efficiency', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Throughput Ratio')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Actions taken\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        action_names = ['Do Nothing', 'Add Chute', 'Reroute', 'Priority Process']\n",
    "        action_counts = [base_env.history['actions'].count(i) for i in range(4)]\n",
    "        bars = plt.bar(action_names, action_counts, color=['gray', 'blue', 'purple', 'cyan'])\n",
    "        plt.title('Actions Taken by Agent', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, action_counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    str(count), ha='center', va='bottom')\n",
    "    else:\n",
    "        # Create placeholder plots when no history is available\n",
    "        for i in range(1, 5):\n",
    "            ax = plt.subplot(3, 3, i)\n",
    "            plt.text(0.5, 0.5, 'No Training History\\nAvailable', \n",
    "                    ha='center', va='center', transform=ax.transAxes,\n",
    "                    fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "            plt.title(['Load Over Time', 'Queue Length', 'Throughput', 'Actions'][i-1], \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 5. Reward progression\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    if has_history and len(base_env.history['rewards']) > 0:\n",
    "        plt.plot(base_env.history['rewards'], color='darkgreen', linewidth=2, alpha=0.7)\n",
    "        # Only plot moving average if we have enough data points\n",
    "        if len(base_env.history['rewards']) >= 10:\n",
    "            window_size = min(10, len(base_env.history['rewards']))\n",
    "            moving_avg = np.convolve(base_env.history['rewards'], np.ones(window_size)/window_size, mode='valid')\n",
    "            plt.plot(range(window_size-1, len(base_env.history['rewards'])), moving_avg, \n",
    "                     color='darkgreen', linewidth=3, label=f'Moving Average ({window_size})')\n",
    "            plt.legend()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No Reward History\\nAvailable', \n",
    "                ha='center', va='center', transform=ax5.transAxes,\n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    plt.title('Reward Progression', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Load vs Delays scatter\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    if has_history and len(base_env.history['load']) > 0:\n",
    "        plt.scatter(base_env.history['load'], base_env.history['delays'], \n",
    "                   alpha=0.6, c=base_env.history['queue'], cmap='viridis')\n",
    "        plt.colorbar(label='Queue Length')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No History Data\\nAvailable', \n",
    "                ha='center', va='center', transform=ax6.transAxes,\n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    plt.title('Load vs Delays (colored by Queue)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Chute Load')\n",
    "    plt.ylabel('Average Delay')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Performance metrics\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    if has_history:\n",
    "        metrics = ['Avg Load', 'Avg Queue', 'Avg Throughput', 'Avg Delay']\n",
    "        values = [\n",
    "            np.mean(base_env.history['load']),\n",
    "            np.mean(base_env.history['queue']),\n",
    "            np.mean(base_env.history['throughput']),\n",
    "            np.mean(base_env.history['delays'])\n",
    "        ]\n",
    "        colors = ['red', 'orange', 'green', 'purple']\n",
    "        bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{value:.2f}', ha='center', va='bottom')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No Performance\\nData Available', \n",
    "                ha='center', va='center', transform=ax7.transAxes,\n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    plt.title('Average Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 8. Test episode results\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    episodes = [r['episode'] for r in test_results]\n",
    "    rewards = [r['total_reward'] for r in test_results]\n",
    "    plt.bar(episodes, rewards, color='lightblue', edgecolor='navy', alpha=0.7)\n",
    "    plt.title('Test Episode Performance', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. System efficiency heatmap\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    # Create efficiency matrix (load vs queue)\n",
    "    load_bins = np.linspace(0, 2, 10)\n",
    "    queue_bins = np.linspace(0, 100, 10)\n",
    "    efficiency_matrix = np.zeros((len(load_bins)-1, len(queue_bins)-1))\n",
    "    \n",
    "    for i, load in enumerate(base_env.history['load']):\n",
    "        for j, queue in enumerate(base_env.history['queue']):\n",
    "            load_idx = np.digitize(load, load_bins) - 1\n",
    "            queue_idx = np.digitize(queue, queue_bins) - 1\n",
    "            if 0 <= load_idx < len(load_bins)-1 and 0 <= queue_idx < len(queue_bins)-1:\n",
    "                efficiency_matrix[load_idx, queue_idx] += base_env.history['throughput'][i] if i < len(base_env.history['throughput']) else 0\n",
    "    \n",
    "    im = plt.imshow(efficiency_matrix, cmap='RdYlGn', aspect='auto')\n",
    "    plt.colorbar(im, label='Efficiency Score')\n",
    "    plt.title('System Efficiency Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Queue Length Bins')\n",
    "    plt.ylabel('Load Level Bins')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('ðŸ­ Packaging Center Contention Simulation Results', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nðŸ“Š SIMULATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if we have history data\n",
    "    if len(base_env.history['load']) > 0:\n",
    "        print(f\"Total Steps Simulated: {len(base_env.history['load'])}\")\n",
    "        print(f\"Average Load Level: {np.mean(base_env.history['load']):.3f}\")\n",
    "        print(f\"Average Queue Length: {np.mean(base_env.history['queue']):.1f} packages\")\n",
    "        print(f\"Average Throughput: {np.mean(base_env.history['throughput']):.3f}\")\n",
    "        print(f\"Average Delay: {np.mean(base_env.history['delays']):.3f} hours\")\n",
    "        print(f\"Total Reward Earned: {np.sum(base_env.history['rewards']):.2f}\")\n",
    "        print(f\"Peak Load Reached: {np.max(base_env.history['load']):.3f}\")\n",
    "        print(f\"Maximum Queue Length: {np.max(base_env.history['queue']):.1f} packages\")\n",
    "        \n",
    "        action_summary = {\n",
    "            0: \"Do Nothing\",\n",
    "            1: \"Add Chute\", \n",
    "            2: \"Reroute Packages\",\n",
    "            3: \"Priority Processing\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAgent Action Distribution:\")\n",
    "        for action, count in enumerate([base_env.history['actions'].count(i) for i in range(4)]):\n",
    "            percentage = (count / len(base_env.history['actions'])) * 100 if len(base_env.history['actions']) > 0 else 0\n",
    "            print(f\"  {action_summary[action]}: {count} times ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No simulation history available - this may be due to using vectorized environment.\")\n",
    "        print(\"Summary from test episodes:\")\n",
    "        if test_results:\n",
    "            avg_reward = np.mean([r['total_reward'] for r in test_results])\n",
    "            avg_steps = np.mean([r['steps'] for r in test_results])\n",
    "            print(f\"Average Episode Reward: {avg_reward:.2f}\")\n",
    "            print(f\"Average Episode Length: {avg_steps:.1f} steps\")\n",
    "            print(f\"Test Episodes Completed: {len(test_results)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the simulation\n",
    "    model, env, results = run_simulation(total_timesteps=10000, test_episodes=5)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Simulation completed successfully!\")\n",
    "    print(\"Check the visualizations above for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # âœ… Manual simulation for tracking history\n",
    "    obs = base_env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, _ = base_env.step(action)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
